{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Inspect Expert Output\n",
    "\n",
    "This notebook shows how to visualize results of expert after running it locally.\n",
    "\n",
    "##### Steps\n",
    "\n",
    "Follow this [guide](https://ln2697.github.io/lead/docs/data_collection.html) to produce some local data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext ipython_beartype\n",
    "%beartype\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from lead.common.visualizer import Visualizer\n",
    "from lead.data_loader.carla_dataset import CARLAData\n",
    "from lead.training.config_training import TrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Setup dataloader and point the root to the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = TrainingConfig()\n",
    "training_config.visualize_dataset = True\n",
    "training_config.use_sensor_perburtation = False\n",
    "\n",
    "training_config.carla_root = \"data/expert_debug\"\n",
    "\n",
    "dataset = CARLAData(root=training_config.carla_data, config=training_config, random=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Visualize one sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(dataset)):  # You can change this index to test different samples\n",
    "    sample = dataset[index]\n",
    "    sample = torch.utils.data._utils.collate.default_collate([sample])  # Collate into a batch of size 1\n",
    "\n",
    "    visualizer = Visualizer(config=training_config, data=sample, prediction=None)\n",
    "    image = visualizer.visualize_training_labels()\n",
    "    stacked_perspective = visualizer.stacked_perspectives\n",
    "    stacked_perspective = cv2.cvtColor(stacked_perspective, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(f\"outputs/visualization/stacked_perspectives_{index:04d}.png\", stacked_perspective)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10d187f",
   "metadata": {},
   "source": [
    "Create an animated WebP from the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c6efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "def create_webp_gif(image_dir, output_path, duration=500):\n",
    "    \"\"\"\n",
    "    Create an animated WebP from images, using only the left half of each image.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing the images\n",
    "        output_path: Path to save the output WebP file\n",
    "        duration: Duration of each frame in milliseconds\n",
    "    \"\"\"\n",
    "    image_dir = Path(image_dir)\n",
    "    image_files = sorted(image_dir.glob(\"stacked_perspectives_*.png\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    frames = []\n",
    "    for img_path in image_files[200:500]:  # Limit to first 500 images\n",
    "        img = Image.open(img_path)\n",
    "        # Take only the left half\n",
    "        width, height = img.size\n",
    "        left_half = img.crop((0, 0, width // 2, height))\n",
    "        frames.append(left_half)\n",
    "    \n",
    "    if frames:\n",
    "        # Save as animated WebP\n",
    "        frames[0].save(\n",
    "            output_path,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=duration,\n",
    "            loop=0,\n",
    "            compression=9,\n",
    "            quality=60\n",
    "        )\n",
    "        print(f\"Created animated WebP with {len(frames)} frames: {output_path}\")\n",
    "    \n",
    "# Create the animated WebP\n",
    "create_webp_gif(\"outputs/visualization\", \"outputs/visualization.webp\", duration=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lead",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
