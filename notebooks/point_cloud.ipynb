{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "from lead.common import common_utils\n",
    "from lead.common.constants import (\n",
    "    SEMANTIC_SEGMENTATION_CONVERTER,\n",
    "    TRANSFUSER_SEMANTIC_COLORS,\n",
    "    CameraPointCloudIndex,\n",
    "    CarlaSemanticSegmentationClass,\n",
    "    TransfuserSemanticSegmentationClass,\n",
    ")\n",
    "\n",
    "for frame_id in range(200, 500):\n",
    "    root = \"data/expert_debug/data/debug_routes/Town15_Rep-1_1_town15_construction_route0_01_15_12_48_52\"\n",
    "\n",
    "    pc_path = f\"{root}/camera_pc/{str(frame_id).zfill(4)}.npz\"\n",
    "    bb_path = f\"{root}/bboxes/{str(frame_id).zfill(4)}.pkl\"\n",
    "\n",
    "    points_ego = np.load(pc_path)[\"arr_0\"]\n",
    "    boxes = common_utils.read_pickle(bb_path)\n",
    "\n",
    "    # 3D visualization with Open3D\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "    # Ensure points_ego is in the correct shape (N, 3) and dtype\n",
    "    if points_ego.shape[0] == 3 or points_ego.shape[0] == 4:\n",
    "        # If shape is (3, N) or (4, N), transpose to (N, 3) or (N, 4)\n",
    "        points_ego = points_ego.T\n",
    "\n",
    "    # Extract xyz coordinates and semantic class using the proper indices\n",
    "    points_xyz = points_ego[:, [CameraPointCloudIndex.X, CameraPointCloudIndex.Y, CameraPointCloudIndex.Z]].astype(np.float64)\n",
    "    carla_semantic_classes = points_ego[:, CameraPointCloudIndex.UNREAL_SEMANTICS_ID].astype(np.int32)\n",
    "\n",
    "    # Flip Y-axis to convert from CARLA to Open3D coordinate system\n",
    "    points_xyz[:, 1] = -points_xyz[:, 1]\n",
    "\n",
    "    # Convert CARLA semantic classes to TransFuser semantic classes\n",
    "    transfuser_semantic_classes = np.array(\n",
    "        [\n",
    "            SEMANTIC_SEGMENTATION_CONVERTER.get(\n",
    "                CarlaSemanticSegmentationClass(c), TransfuserSemanticSegmentationClass.UNLABELED\n",
    "            )\n",
    "            for c in carla_semantic_classes\n",
    "        ],\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "\n",
    "    # Filter out points that are too far away (> 100m)\n",
    "    distances = np.linalg.norm(points_xyz, axis=1)\n",
    "    mask = distances <= 100.0\n",
    "    points_xyz = points_xyz[mask]\n",
    "    transfuser_semantic_classes = transfuser_semantic_classes[mask]\n",
    "\n",
    "    # Convert TransFuser colors from RGB (0-255) to normalized [0-1] for Open3D\n",
    "    TRANSFUSER_SEMANTIC_COLORS[TransfuserSemanticSegmentationClass.UNLABELED] = [60, 60, 60]  # Grey unlabeled\n",
    "    transfuser_colors_normalized = {\n",
    "        k: [v[0] / 255.0, v[1] / 255.0, v[2] / 255.0] for k, v in TRANSFUSER_SEMANTIC_COLORS.items()\n",
    "    }\n",
    "\n",
    "    # Assign colors to points based on TransFuser semantic class\n",
    "    colors = np.array(\n",
    "        [\n",
    "            transfuser_colors_normalized.get(\n",
    "                TransfuserSemanticSegmentationClass(int(c)),\n",
    "                [0.0, 0.0, 0.0],  # Black for unlabeled\n",
    "            )\n",
    "            for c in transfuser_semantic_classes\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pcd.points = o3d.utility.Vector3dVector(points_xyz)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    # Create line set for bounding boxes - SINGLE LOOP\n",
    "    all_points = []\n",
    "    lines_list = []\n",
    "    colors_list = []\n",
    "\n",
    "    for box in boxes:\n",
    "        # Get offset BEFORE adding points\n",
    "        offset = len(all_points)\n",
    "\n",
    "        # Create box corners\n",
    "        extent = np.array(box[\"extent\"])\n",
    "        center = np.array(box[\"position\"])\n",
    "        yaw = box[\"yaw\"]\n",
    "\n",
    "        # Flip Y-axis for bounding box center\n",
    "        center[1] = -center[1]\n",
    "        # Flip yaw angle for Y-axis flip\n",
    "        yaw = -yaw\n",
    "\n",
    "        # Define the 8 corners of the box in local coordinates\n",
    "        corners_local = np.array(\n",
    "            [\n",
    "                [-extent[0], -extent[1], -extent[2]],\n",
    "                [extent[0], -extent[1], -extent[2]],\n",
    "                [extent[0], extent[1], -extent[2]],\n",
    "                [-extent[0], extent[1], -extent[2]],\n",
    "                [-extent[0], -extent[1], extent[2]],\n",
    "                [extent[0], -extent[1], extent[2]],\n",
    "                [extent[0], extent[1], extent[2]],\n",
    "                [-extent[0], extent[1], extent[2]],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Rotation matrix\n",
    "        R = np.array([[np.cos(yaw), -np.sin(yaw), 0], [np.sin(yaw), np.cos(yaw), 0], [0, 0, 1]])\n",
    "\n",
    "        # Transform corners to world coordinates\n",
    "        corners_world = (R @ corners_local.T).T + center\n",
    "\n",
    "        # Add corners NOW\n",
    "        all_points.extend(corners_world)\n",
    "\n",
    "        # Define the 12 edges of the box\n",
    "        edges = [\n",
    "            [0, 1],\n",
    "            [1, 2],\n",
    "            [2, 3],\n",
    "            [3, 0],  # bottom face\n",
    "            [4, 5],\n",
    "            [5, 6],\n",
    "            [6, 7],\n",
    "            [7, 4],  # top face\n",
    "            [0, 4],\n",
    "            [1, 5],\n",
    "            [2, 6],\n",
    "            [3, 7],  # vertical edges\n",
    "        ]\n",
    "\n",
    "        # Color based on class\n",
    "        if box.get(\"class\") == \"ego_car\":\n",
    "            color = [0, 1, 0]  # Green for ego\n",
    "        elif box.get(\"transfuser_semantics_id\") == TransfuserSemanticSegmentationClass.VEHICLE:\n",
    "            color = [1.0, 0.5, 0]  # Red for vehicles\n",
    "        elif box.get(\"transfuser_semantics_id\") == TransfuserSemanticSegmentationClass.PEDESTRIAN:\n",
    "            color = [0, 0, 1]  # Blue for pedestrians\n",
    "        else:\n",
    "            color = [1, 1, 0]  # Yellow for others\n",
    "\n",
    "        # Add edges with offset\n",
    "        for edge in edges:\n",
    "            lines_list.append([offset + edge[0], offset + edge[1]])\n",
    "            colors_list.append(color)\n",
    "\n",
    "    # Create line set geometry\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(np.array(all_points))\n",
    "    line_set.lines = o3d.utility.Vector2iVector(np.array(lines_list))\n",
    "    line_set.colors = o3d.utility.Vector3dVector(np.array(colors_list))\n",
    "\n",
    "    # Use OffscreenRenderer for proper offscreen rendering\n",
    "    output_path = f\"outputs/point_cloud_visualization/{str(frame_id).zfill(4)}.png\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    width, height = 640, 659\n",
    "    renderer = o3d.visualization.rendering.OffscreenRenderer(width, height)\n",
    "\n",
    "    # Setup material\n",
    "    material = o3d.visualization.rendering.MaterialRecord()\n",
    "    material.shader = \"defaultUnlit\"\n",
    "    material.point_size = 1.5  # Make points smaller\n",
    "\n",
    "    # Add point cloud\n",
    "    renderer.scene.add_geometry(\"pcd\", pcd, material)\n",
    "\n",
    "    # Add bounding boxes\n",
    "    material_lines = o3d.visualization.rendering.MaterialRecord()\n",
    "    material_lines.shader = \"unlitLine\"\n",
    "    material_lines.line_width = 1.5\n",
    "    renderer.scene.add_geometry(\"boxes\", line_set, material_lines)\n",
    "\n",
    "    # Set camera view relative to ego\n",
    "    camera_pos = np.array([-40, 30, 60])  # Behind and above ego\n",
    "    look_at = np.array([0, 0, 0])  # Look at ego\n",
    "    up_vector = np.array([0, 0, 1])  # Z is up\n",
    "\n",
    "    renderer.setup_camera(60.0, look_at, camera_pos, up_vector)\n",
    "\n",
    "    # Set background color\n",
    "    renderer.scene.set_background([1.0, 1.0, 1.0, 1.0])  # White\n",
    "    renderer.scene.view.set_post_processing(False)\n",
    "\n",
    "    # Render and save\n",
    "    img = renderer.render_to_image()\n",
    "    o3d.io.write_image(output_path, img)\n",
    "    del renderer\n",
    "    del pcd\n",
    "    del line_set\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02127cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "def create_webp_gif(image_dir, output_path, duration=500):\n",
    "    \"\"\"\n",
    "    Create an animated WebP from images, using only the left half of each image.\n",
    "    \n",
    "    Args:\n",
    "        image_dir: Directory containing the images\n",
    "        output_path: Path to save the output WebP file\n",
    "        duration: Duration of each frame in milliseconds\n",
    "    \"\"\"\n",
    "    image_dir = Path(image_dir)\n",
    "    image_files = sorted(image_dir.glob(\"*.png\"))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No images found in {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    frames = []\n",
    "    for img_path in image_files:\n",
    "        img = Image.open(img_path)\n",
    "        frames.append(img)\n",
    "    \n",
    "    if frames:\n",
    "        # Save as animated WebP\n",
    "        frames[0].save(\n",
    "            output_path,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=duration,\n",
    "            loop=0,\n",
    "            compression=9,\n",
    "            quality=30\n",
    "        )\n",
    "        print(f\"Created animated WebP with {len(frames)} frames: {output_path}\")\n",
    "    \n",
    "# Create the animated WebP\n",
    "create_webp_gif(\"outputs/point_cloud_visualization\", \"outputs/point_cloud_visualization.webp\", duration=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lead",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
